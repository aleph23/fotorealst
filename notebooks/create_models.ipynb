{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import vgg19 \n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np \n",
    "from tqdm import tqdm \n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make conv layer class for easy writing in next class\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1):\n",
    "        super().__init__()\n",
    "        # We have to keep the image size same\n",
    "        num_pad = int(np.floor(kernel_size / 2))\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=num_pad)\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleneckBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Bottleneck layer similar to resnet bottleneck layer. InstanceNorm is used\n",
    "    instead of BatchNorm because when we want to generate images, we normalize\n",
    "    all the images independently. \n",
    "    \n",
    "    (In batch norm you compute mean and std over complete batch, while in instance \n",
    "    norm you compute mean and std of each image independently). The reason for \n",
    "    doing this is, the generated images are independent of each other, so we should\n",
    "    not normalize them using a common statistic.\n",
    "    \n",
    "    If you confused about the bottleneck architecture refer to the official pytorch\n",
    "    resnet implementation and paper.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1):\n",
    "        super().__init__()\n",
    "        self.in_c = in_channels\n",
    "        self.out_c = out_channels\n",
    "        \n",
    "        self.identity_block = nn.Sequential(\n",
    "            ConvLayer(in_channels, out_channels//4, kernel_size=1, stride=1),\n",
    "            nn.InstanceNorm2d(out_channels//4),\n",
    "            nn.ReLU(),\n",
    "            ConvLayer(out_channels//4, out_channels//4, kernel_size, stride=stride),\n",
    "            nn.InstanceNorm2d(out_channels//4),\n",
    "            nn.ReLU(),\n",
    "            ConvLayer(out_channels//4, out_channels, kernel_size=1, stride=1),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.shortcut = nn.Sequential(\n",
    "            ConvLayer(in_channels, out_channels, 1, stride),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.identity_block(x)\n",
    "        if self.in_c == self.out_c:\n",
    "            residual = x\n",
    "        else:\n",
    "            residual = self.shortcut(x)\n",
    "        out += residual\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not used in the implementation\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, scale_factor, mode='bilinear'):\n",
    "        super().__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.mode = mode\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1)\n",
    "        self.norm = nn.InstanceNorm2d(out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = F.interpolate(out, scale_factor=self.scale_factor, mode=self.mode, align_corners=False)\n",
    "        out = self.norm(out)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for HRNet\n",
    "def conv_down(in_c, out_c, stride=2):\n",
    "    return nn.Conv2d(in_c, out_c, kernel_size=3, stride=stride, padding=1)\n",
    "\n",
    "def upsample(scale_factor):\n",
    "    return nn.Upsample(scale_factor=scale_factor, mode='bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HRNet(nn.Module):\n",
    "    \"\"\"\n",
    "    For model reference see Figure 2 of the paper https://arxiv.org/pdf/1904.11617v1.pdf.\n",
    "    \n",
    "    Naming convention used.\n",
    "    I refer to vertical layers as a single layer, so from left to right we have 8 layers\n",
    "    excluding the input image.\n",
    "    E.g. layer 1 contains the 500x500x16 block\n",
    "         layer 2 contains 500x500x32 and 250x250x32 blocks and so on\n",
    "    \n",
    "    self.layer{x}_{y}:\n",
    "        x :- the layer number, as explained above\n",
    "        y :- the index number for that function starting from 1. So if layer 3 has two\n",
    "             downsample functions I write them as `downsample3_1`, `downsample3_2`\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1_1 = BottleneckBlock(3, 16)\n",
    "        \n",
    "        self.layer2_1 = BottleneckBlock(16, 32)\n",
    "        self.downsample2_1 = conv_down(16, 32)\n",
    "        \n",
    "        self.layer3_1 = BottleneckBlock(32, 32)\n",
    "        self.layer3_2 = BottleneckBlock(32, 32)\n",
    "        self.downsample3_1 = conv_down(32, 32)\n",
    "        self.downsample3_2 = conv_down(32, 32, stride=4)\n",
    "        self.downsample3_3 = conv_down(32, 32)\n",
    "        \n",
    "        self.layer4_1 = BottleneckBlock(64, 64)\n",
    "        self.layer5_1 = BottleneckBlock(192, 64)\n",
    "        self.layer6_1 = BottleneckBlock(64, 32)\n",
    "        self.layer7_1 = BottleneckBlock(32, 16)\n",
    "        self.layer8_1 = conv_down(16, 3, stride=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        map1_1 = self.layer1_1(x)\n",
    "        \n",
    "        map2_1 = self.layer2_1(map1_1)\n",
    "        map2_2 = self.downsample2_1(map1_1)\n",
    "        \n",
    "        map3_1 = torch.cat((self.layer3_1(map2_1), upsample(map2_2, 2)), 1)\n",
    "        map3_2 = torch.cat((self.downsample3_1(map2_1), self.layer3_2(map2_2)), 1)\n",
    "        map3_3 = torch.cat((self.downsample3_2(map2_1), self.downsample3_3(map2_2)), 1)\n",
    "        \n",
    "        map4_1 = torch.cat((self.layer4_1(map3_1), upsample(map3_2, 2), upsample(map3_3, 4)), 1)\n",
    "        \n",
    "        out = self.layer5_1(map4_1)\n",
    "        out = self.layer6_1(out)\n",
    "        out = self.layer7_1(out)\n",
    "        out = self.layer8_1(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create utility functions for image loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path, size=None):\n",
    "    \"\"\"\n",
    "    Resize img to size, size should be int and also normalize the\n",
    "    image using imagenet_stats\n",
    "    \"\"\"\n",
    "    img = Image.open(path)\n",
    "\n",
    "    if size is not None:\n",
    "        img = img.resize((size, size))\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "    \n",
    "    img = transform(img).unsqueeze(0)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im_convert(img):\n",
    "    \"\"\"\n",
    "    Convert img from pytorch tensor to numpy array, so we can plot it.\n",
    "    It follows the standard method of denormalizing the img and clipping\n",
    "    the outputs\n",
    "    \n",
    "    Input:\n",
    "        img :- (batch, channel, height, width)\n",
    "    Output:\n",
    "        img :- (height, width, channel)\n",
    "    \"\"\"\n",
    "    img = img.to('cpu').clone().detach()\n",
    "    img = img.numpy().squeeze(0)\n",
    "    img = img.transpose(1, 2, 0)\n",
    "    img = img * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
    "    img = img.clip(0, 1)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(img, model, layers=None):\n",
    "    \"\"\"\n",
    "    Use VGG19 to extract features from the intermediate layers.\n",
    "    \"\"\"\n",
    "    if layers is None:\n",
    "        layers = {\n",
    "            '0': 'conv1_1',   # style layer\n",
    "            '5': 'conv2_1',   # style layer\n",
    "            '10': 'conv3_1',  # style layer\n",
    "            '19': 'conv4_1',  # style layer\n",
    "            '28': 'conv5_1',  # style layer\n",
    "            \n",
    "            '21': 'conv4_2'   # content layer\n",
    "        }\n",
    "    \n",
    "    features = {}\n",
    "    x = img\n",
    "    for name, layer in model._modules.items():\n",
    "        x = layer(x)\n",
    "        if name in layers:\n",
    "            features[layers[name]] = x\n",
    "            \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gram_matrix(img):\n",
    "    \"\"\"\n",
    "    Compute the gram matrix by converting to 2D tensor and doing dot product\n",
    "    img: (batch, channel, height, width)\n",
    "    \"\"\"\n",
    "    b, c, h, w = img.size()\n",
    "    img = img.view(b*c, h*w)\n",
    "    gram = torch.mm(img, img.t())\n",
    "    return gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write style_transfer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to train_model.ipynb for continuation of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data, place your images in the img folder and name it as content.png and style.png\n",
    "# You can also input your images directly (for .py script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.img_root = 'src/imgs'\n",
    "        self.content_img = 'content.png'\n",
    "        self.style_img = 'style.png'\n",
    "        self.use_batch = False\n",
    "        self.bs = 16\n",
    "        self.use_gpu = True\n",
    "        \n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.use_gpu:\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        raise Exception('GPU is not available')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "# Load VGG19 features\n",
    "vgg = vgg19(pretrained=True).features\n",
    "vgg = vgg.to(device)\n",
    "# We don't want to train VGG\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad_(False)\n",
    "    \n",
    "# Load style net\n",
    "style_net = HRNet()\n",
    "style_net = style_net.to(device)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "content_img = load_image(os.path.join(args.img_root, args.content_img), size=500)\n",
    "content_img = content_img.to(device)\n",
    "\n",
    "style_img = load_image(os.path.join(args.img_root, args.style_img))\n",
    "style_img = style_img.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 500, 500]), torch.Size([1, 3, 800, 800]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_img.size(), style_img.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
