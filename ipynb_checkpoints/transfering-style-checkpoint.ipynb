{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432ba4b0",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 13.137401,
     "end_time": "2022-02-15T09:35:31.079780",
     "exception": false,
     "start_time": "2022-02-15T09:35:17.942379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/usr/src/'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3311ef6a",
   "metadata": {
    "papermill": {
     "duration": 0.049851,
     "end_time": "2022-02-15T09:35:31.196774",
     "exception": false,
     "start_time": "2022-02-15T09:35:31.146923",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---------------------------------------\n",
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4295364c",
   "metadata": {
    "papermill": {
     "duration": 0.050483,
     "end_time": "2022-02-15T09:35:31.298842",
     "exception": false,
     "start_time": "2022-02-15T09:35:31.248359",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "![](https://miro.medium.com/max/1200/1*XI3beonBnOwp-y5BwNOqCw.gif)\n",
    "\n",
    "Picture Credit: https://miro.medium.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e952c45",
   "metadata": {
    "papermill": {
     "duration": 0.050779,
     "end_time": "2022-02-15T09:35:31.400185",
     "exception": false,
     "start_time": "2022-02-15T09:35:31.349406",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Neural Style Transfer**\n",
    "\n",
    "> Neural Style Transfer (NST) refers to a class of software algorithms that manipulate digital images, or videos, in order to adopt the appearance or visual style of another image. NST algorithms are characterized by their use of deep neural networks for the sake of image transformation. Common uses for NST are the creation of artificial artwork from photographs, for example by transferring the appearance of famous paintings to user-supplied photographs. Several notable mobile apps use NST techniques for this purpose, including DeepArt and Prisma. This method has been used by artists and designers around the globe to develop new artwork based on existent style(s).\n",
    "\n",
    "Ref: https://en.wikipedia.org/wiki/Neural_Style_Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591e84eb",
   "metadata": {
    "papermill": {
     "duration": 0.049476,
     "end_time": "2022-02-15T09:35:31.502131",
     "exception": false,
     "start_time": "2022-02-15T09:35:31.452655",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Style transfer means that when a content image and a style image are given, the outline and shape of the image are similar to the content image, and the color or texture is changed to be similar to the style image.\n",
    "\n",
    "By separating content and style, you can mix content and style of different images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8e79f8",
   "metadata": {
    "papermill": {
     "duration": 0.049089,
     "end_time": "2022-02-15T09:35:31.600986",
     "exception": false,
     "start_time": "2022-02-15T09:35:31.551897",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "A pre-trained VGG19 Net is used as a model to extract content and style. It then uses the losses of the content and style to iteratively update the target image until the desired result is achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4e84b1b",
   "metadata": {
    "papermill": {
     "duration": 4.527866,
     "end_time": "2022-02-15T09:35:36.178231",
     "exception": false,
     "start_time": "2022-02-15T09:35:31.650365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Tensor' from 'torch' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6608/2207751670.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/optim/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \"\"\"\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0madadelta\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdadelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0madagrad\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdagrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0madam\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/optim/adadelta.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Tensor' from 'torch' (unknown location)"
     ]
    }
   ],
   "source": [
    "# import resources\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import requests\n",
    "from torchvision import transforms, models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5ecca6",
   "metadata": {
    "papermill": {
     "duration": 0.049735,
     "end_time": "2022-02-15T09:35:36.278637",
     "exception": false,
     "start_time": "2022-02-15T09:35:36.228902",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "------------------------------------------------\n",
    "# 1. Load in model\n",
    "\n",
    "VGG19 is divided into two parts.\n",
    "\n",
    "* vgg19.features: All convolutional layers and pooling layers\n",
    "* vgg19.classifier: The last three linear layers are the classifier layer.\n",
    "\n",
    "We only need the features part. And \"freeze\" so that the weight is not updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6adb81",
   "metadata": {
    "papermill": {
     "duration": 21.187586,
     "end_time": "2022-02-15T09:35:57.516040",
     "exception": false,
     "start_time": "2022-02-15T09:35:36.328454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the \"features\" portion of VGG19 (we will not need the \"classifier\" portion)\n",
    "vgg = models.vgg19(pretrained=True).features\n",
    "\n",
    "# freeze all VGG parameters since we're only optimizing the target image\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14d1cc5",
   "metadata": {
    "papermill": {
     "duration": 5.227876,
     "end_time": "2022-02-15T09:36:02.794803",
     "exception": false,
     "start_time": "2022-02-15T09:35:57.566927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# move the model to GPU, if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vgg.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b430d9f8",
   "metadata": {
    "papermill": {
     "duration": 0.052727,
     "end_time": "2022-02-15T09:36:02.898466",
     "exception": false,
     "start_time": "2022-02-15T09:36:02.845739",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "-----------------------------------------------\n",
    "# 2. Load in Content and Style Images\n",
    "\n",
    "Load the content image and style image to be used for style transfer. The load_image function transforms the image and loads it in the form of normalized Tensors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2025b645",
   "metadata": {
    "papermill": {
     "duration": 0.062897,
     "end_time": "2022-02-15T09:36:03.012758",
     "exception": false,
     "start_time": "2022-02-15T09:36:02.949861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_image(img_path, max_size=128, shape=None):\n",
    "    ''' Load in and transform an image, making sure the image\n",
    "       is <= 400 pixels in the x-y dims.'''\n",
    "    if \"http\" in img_path:\n",
    "        response = requests.get(img_path)\n",
    "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    else:\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "    \n",
    "    # large images will slow down processing\n",
    "    if max(image.size) > max_size:\n",
    "        size = max_size\n",
    "    else:\n",
    "        size = max(image.size)\n",
    "    \n",
    "    if shape is not None:\n",
    "        size = shape\n",
    "        \n",
    "    in_transform = transforms.Compose([\n",
    "                        transforms.Resize(size),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                                             (0.229, 0.224, 0.225))])\n",
    "\n",
    "    # discard the transparent, alpha channel (that's the :3) and add the batch dimension\n",
    "    image = in_transform(image)[:3,:,:].unsqueeze(0)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a02467a",
   "metadata": {
    "papermill": {
     "duration": 0.04901,
     "end_time": "2022-02-15T09:36:03.111834",
     "exception": false,
     "start_time": "2022-02-15T09:36:03.062824",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Load the content image and style image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3c398f",
   "metadata": {
    "papermill": {
     "duration": 0.286306,
     "end_time": "2022-02-15T09:36:03.447192",
     "exception": false,
     "start_time": "2022-02-15T09:36:03.160886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load in content and style image\n",
    "content = load_image('/kaggle/input/images-for-style-transfer/Data/Artworks/856047.jpg').to(device)\n",
    "# Resize style to match content, makes code easier\n",
    "style = load_image('/kaggle/input/neural-style-transfer/Style Images/starry_night.jpg').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079dbf91",
   "metadata": {
    "papermill": {
     "duration": 0.058866,
     "end_time": "2022-02-15T09:36:03.557169",
     "exception": false,
     "start_time": "2022-02-15T09:36:03.498303",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# helper function for un-normalizing an image \n",
    "# and converting it from a Tensor image to a NumPy image for display\n",
    "def im_convert(tensor):\n",
    "    \"\"\" Display a tensor as an image. \"\"\"\n",
    "    \n",
    "    image = tensor.to(\"cpu\").clone().detach()\n",
    "    image = image.numpy().squeeze()\n",
    "    image = image.transpose(1,2,0)\n",
    "    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
    "    image = image.clip(0, 1)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ac129f",
   "metadata": {
    "papermill": {
     "duration": 0.572122,
     "end_time": "2022-02-15T09:36:04.177871",
     "exception": false,
     "start_time": "2022-02-15T09:36:03.605749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display the images\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "# content and style ims side-by-side\n",
    "ax1.imshow(im_convert(content))\n",
    "ax2.imshow(im_convert(style))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06e2504",
   "metadata": {
    "papermill": {
     "duration": 0.066672,
     "end_time": "2022-02-15T09:36:04.302575",
     "exception": false,
     "start_time": "2022-02-15T09:36:04.235903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_features(image, model, layers=None):\n",
    "    \"\"\" Run an image forward through a model and get the features for \n",
    "        a set of layers. Default layers are for VGGNet matching Gatys et al (2016)\n",
    "    \"\"\"\n",
    "    \n",
    "    if layers is None:\n",
    "        layers = {'0': 'conv1_1',\n",
    "                  '5': 'conv2_1', \n",
    "                  '10': 'conv3_1', \n",
    "                  '19': 'conv4_1',\n",
    "                  '21': 'conv4_2',  ## content representation\n",
    "                  '28': 'conv5_1'}\n",
    "        \n",
    "    features = {}\n",
    "    x = image\n",
    "    # model._modules is a dictionary holding each module in the model\n",
    "    for name, layer in model._modules.items():\n",
    "        x = layer(x)\n",
    "        if name in layers:\n",
    "            features[layers[name]] = x\n",
    "            \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6f88cf",
   "metadata": {
    "papermill": {
     "duration": 0.058466,
     "end_time": "2022-02-15T09:36:04.417945",
     "exception": false,
     "start_time": "2022-02-15T09:36:04.359479",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "--------------------------------------------------\n",
    "# 3. Gram Matrix\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*VAQs1KSfbysnloPah_fHGQ.gif)\n",
    "\n",
    "Picture Credit: https://miro.medium.com\n",
    "\n",
    "\n",
    "> The matrix expressing the correlation of this Channel is called Gram Matrix. Loss is minimized by defining the difference between this Gram Matrix and the Gram Matrix of the newly created image as a Loss Function. Next, in order to reflect the content, the loss function is calculated in units of pixels from the feature map spit out from each pre-trained CNN. In this way, a new image is created that minimizes the Loss calculated from Style and Loss calculated from Content. \n",
    "\n",
    "https://en.wikipedia.org/wiki/Gram_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9421fb",
   "metadata": {
    "papermill": {
     "duration": 0.114623,
     "end_time": "2022-02-15T09:36:04.599214",
     "exception": false,
     "start_time": "2022-02-15T09:36:04.484591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gram_matrix(tensor):\n",
    "    \"\"\" Calculate the Gram Matrix of a given tensor         \n",
    "    \"\"\"\n",
    "    \n",
    "    # get the batch_size, depth, height, and width of the Tensor\n",
    "    _, d, h, w = tensor.size()\n",
    "    \n",
    "    # reshape so we're multiplying the features for each channel\n",
    "    tensor = tensor.view(d, h * w)\n",
    "    \n",
    "    # calculate the gram matrix\n",
    "    gram = torch.mm(tensor, tensor.t())\n",
    "    \n",
    "    return gram "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bef848e",
   "metadata": {
    "papermill": {
     "duration": 0.057825,
     "end_time": "2022-02-15T09:36:04.717744",
     "exception": false,
     "start_time": "2022-02-15T09:36:04.659919",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The function that extracts the features of a given convolutional layer and computes the Gram Matrix is made. Putting it all together, we extract the features from the image and compute the Gram Matrix for each layer from the style representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4380c739",
   "metadata": {
    "papermill": {
     "duration": 0.166923,
     "end_time": "2022-02-15T09:36:04.943643",
     "exception": false,
     "start_time": "2022-02-15T09:36:04.776720",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get content and style features only once before training\n",
    "content_features = get_features(content, vgg)\n",
    "style_features = get_features(style, vgg)\n",
    "\n",
    "# calculate the gram matrices for each layer of our style representation\n",
    "style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}\n",
    "\n",
    "# create a third \"target\" image and prep it for change\n",
    "# it is a good idea to start off with the target as a copy of our *content* image\n",
    "# then iteratively change its style\n",
    "target = content.clone().requires_grad_(True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0210957",
   "metadata": {
    "papermill": {
     "duration": 0.058649,
     "end_time": "2022-02-15T09:36:05.061601",
     "exception": false,
     "start_time": "2022-02-15T09:36:05.002952",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---------------------------------------------\n",
    "# 4. Define Losses and Weights\n",
    "\n",
    "**Individual Layer Style Weights**\n",
    "\n",
    "You can give the option to weight the style expression in each relevant layer. It is recommended that the layer weight range from 0 to 1. By giving more weight to conv1_1 and conv2_1, more style artifacts can be reflected in the final target image.\n",
    "\n",
    "**Content and Style Weight**\n",
    "\n",
    "Define alpha (content_weight) and beta (style_weight). This ratio affects the style of the final image. It is recommended to leave content_weight = 1 and set style_weight to achieve the desired ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb93aca1",
   "metadata": {
    "papermill": {
     "duration": 0.064726,
     "end_time": "2022-02-15T09:36:05.183246",
     "exception": false,
     "start_time": "2022-02-15T09:36:05.118520",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# weights for each style layer \n",
    "# weighting earlier layers more will result in *larger* style artifacts\n",
    "# notice we are excluding `conv4_2` our content representation\n",
    "style_weights = {'conv1_1': 1,\n",
    "                 'conv2_1': 0.75,\n",
    "                 'conv3_1': 0.2,\n",
    "                 'conv4_1': 0.2,\n",
    "                 'conv5_1': 0.2}\n",
    "\n",
    "content_weight = 1  # alpha\n",
    "style_weight = 1e3  # beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968df8c5",
   "metadata": {
    "papermill": {
     "duration": 0.058063,
     "end_time": "2022-02-15T09:36:05.299033",
     "exception": false,
     "start_time": "2022-02-15T09:36:05.240970",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. Update Target and Calculate Losses\n",
    "\n",
    "**Content Loss**\n",
    "\n",
    "The content loss is calculated as the MSE between the target and the content feature in the 'conv4_2' layer.\n",
    "\n",
    "**Style Loss**\n",
    "\n",
    "The style loss is the loss between the target image and the style image. That is, it refers to the difference between the gram matrix of the style image and the gram matrix of the target image. Loss is calculated using MSE\n",
    "\n",
    "**Total Loss**\n",
    "\n",
    "Finally, the total loss is calculated by summing the style and content losses and weighting them with the specified alpha and beta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc5124f",
   "metadata": {
    "papermill": {
     "duration": 38.238653,
     "end_time": "2022-02-15T09:36:43.594836",
     "exception": false,
     "start_time": "2022-02-15T09:36:05.356183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for displaying the target image, intermittently\n",
    "show_every = 500\n",
    "\n",
    "# iteration hyperparameters\n",
    "optimizer = optim.Adam([target], lr=0.003)\n",
    "steps = 5001  # decide how many iterations to update your image (5000)\n",
    "\n",
    "for ii in range(1, steps+1):\n",
    "    \n",
    "    # get the features from your target image\n",
    "    target_features = get_features(target, vgg)\n",
    "    \n",
    "    # the content loss\n",
    "    content_loss = torch.mean((target_features['conv4_2'] - content_features['conv4_2'])**2)\n",
    "    \n",
    "    # the style loss\n",
    "    # initialize the style loss to 0\n",
    "    style_loss = 0\n",
    "    # then add to it for each layer's gram matrix loss\n",
    "    for layer in style_weights:\n",
    "        # get the \"target\" style representation for the layer\n",
    "        target_feature = target_features[layer]\n",
    "        target_gram = gram_matrix(target_feature)\n",
    "        _, d, h, w = target_feature.shape\n",
    "        # get the \"style\" style representation\n",
    "        style_gram = style_grams[layer]\n",
    "        # the style loss for one layer, weighted appropriately\n",
    "        layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2)\n",
    "        # add to the style loss\n",
    "        style_loss += layer_style_loss / (d * h * w)\n",
    "        \n",
    "    # calculate the *total* loss\n",
    "    total_loss = content_weight * content_loss + style_weight * style_loss\n",
    "    \n",
    "    # update your target image\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # display intermediate images and print the loss\n",
    "    if  ii % show_every == 0:\n",
    "        print('Total loss: ', total_loss.item())\n",
    "        plt.imshow(im_convert(target))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41ad790",
   "metadata": {
    "papermill": {
     "duration": 0.189435,
     "end_time": "2022-02-15T09:36:43.964168",
     "exception": false,
     "start_time": "2022-02-15T09:36:43.774733",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "-----------------------------------------------------------------\n",
    "# 6. Check the last result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72d5c30",
   "metadata": {
    "papermill": {
     "duration": 0.834965,
     "end_time": "2022-02-15T09:36:44.989698",
     "exception": false,
     "start_time": "2022-02-15T09:36:44.154733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display content and final, target image\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "ax1.imshow(im_convert(content))\n",
    "ax2.imshow(im_convert(target))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 94.629355,
   "end_time": "2022-02-15T09:36:46.028377",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-15T09:35:11.399022",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "260e7d02d68944c889c48ee2359b90f7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_7b0095ef03f742d3938a8573c86d0eec",
        "IPY_MODEL_733f06eab38b4281910d0b4bf0e6ed12",
        "IPY_MODEL_d04c83da8fbc497a88d98b1e3144a0ba"
       ],
       "layout": "IPY_MODEL_c0823fcc931543c492fb09880c129e1a"
      }
     },
     "265b050ea9ed4494938553079ee73bd0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5e774823da71428ba9194e1194e66655": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "733f06eab38b4281910d0b4bf0e6ed12": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_265b050ea9ed4494938553079ee73bd0",
       "max": 574673361,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_bf68452a526849dc9a73c2f121d054ee",
       "value": 574673361
      }
     },
     "7b0095ef03f742d3938a8573c86d0eec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b82753e413904b03acbdcca2bd383352",
       "placeholder": "​",
       "style": "IPY_MODEL_a2497dcc20904dca8c3a5e9cc4d7c6d7",
       "value": "100%"
      }
     },
     "a2497dcc20904dca8c3a5e9cc4d7c6d7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b82753e413904b03acbdcca2bd383352": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bf68452a526849dc9a73c2f121d054ee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "c0823fcc931543c492fb09880c129e1a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d04c83da8fbc497a88d98b1e3144a0ba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5e774823da71428ba9194e1194e66655",
       "placeholder": "​",
       "style": "IPY_MODEL_f47254f9856742d89271787f1459c72e",
       "value": " 548M/548M [00:19&lt;00:00, 31.6MB/s]"
      }
     },
     "f47254f9856742d89271787f1459c72e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
